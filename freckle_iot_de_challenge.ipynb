{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('DEChallenge').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and explore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read source data\n",
    "loc_event_df = spark.read.json('location-data-sample/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- action: string (nullable = true)\n",
      " |-- api_key: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- beacon_major: long (nullable = true)\n",
      " |-- beacon_minor: long (nullable = true)\n",
      " |-- beacon_uuid: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- community: string (nullable = true)\n",
      " |-- community_code: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- county_code: string (nullable = true)\n",
      " |-- event_time: long (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      " |-- horizontal_accuracy: double (nullable = true)\n",
      " |-- idfa: string (nullable = true)\n",
      " |-- idfa_hash_alg: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- user_ip: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loc_event_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238211"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_event_df.select('idfa').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8754673"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_event_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is the max, min, avg, std deviation of the number of location events per IDFA? We define a location event to be one record in the sample file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count of location events per idfa\n",
    "loc_event_df_idfa = loc_event_df.groupBy('idfa').count()\n",
    "loc_event_df_idfa = loc_event_df_idfa.withColumnRenamed('count', 'num_loc_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                idfa|num_loc_events|\n",
      "+--------------------+--------------+\n",
      "|6bc2a6e1-c710-4dd...|           158|\n",
      "|3a8ddabb-f98e-46b...|             6|\n",
      "|e118f5a2-7949-429...|            30|\n",
      "|5b9785d5-529d-49f...|           362|\n",
      "|0376307f-d608-452...|            73|\n",
      "|ad2aad6b-6af3-4eb...|            29|\n",
      "|7360e2a4-9d0b-4fa...|            34|\n",
      "|aad8e6c9-ae62-448...|            88|\n",
      "|0c837a76-54f9-4d4...|            95|\n",
      "|16c2c602-3f0c-4ce...|            15|\n",
      "|789864f3-01d1-410...|           248|\n",
      "|e2af50ab-61eb-423...|           107|\n",
      "|c940218f-906e-401...|             9|\n",
      "|6d866497-b0ab-498...|            42|\n",
      "|ace6dd48-1ac3-48e...|            32|\n",
      "|f92c8e26-a827-485...|           158|\n",
      "|4d88225c-97ea-413...|            45|\n",
      "|fcf44327-9fd7-44e...|           165|\n",
      "|32444aa4-67b8-42c...|            35|\n",
      "|6ad974b0-4de7-4b3...|            30|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loc_event_df_idfa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (max, min, avg, stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+----------------+------------------+\n",
      "|max_loc_events|min_loc_events|  avg_loc_events|    std_loc_events|\n",
      "+--------------+--------------+----------------+------------------+\n",
      "|         15979|             1|36.7517578953113|118.61139276213812|\n",
      "+--------------+--------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#max, min, avg, stddev\n",
    "loc_event_df_idfa.select([max('num_loc_events').alias('max_loc_events'),\n",
    "                          min('num_loc_events').alias('min_loc_events'),\n",
    "                          avg('num_loc_events').alias('avg_loc_events'),\n",
    "                          stddev('num_loc_events').alias('std_loc_events')\n",
    "                         ]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Produce geohashes for all coordinates in a new RDD or DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2763239"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count distinct geohash values\n",
    "loc_event_df.select('geohash').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767342"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab unique lat, lng pair\n",
    "coord_df = loc_event_df.select(['lat', 'lng']).distinct()\n",
    "#count unique lat,lng pair\n",
    "coord_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the unique geohash count (2,763,239) and coordinate count (2,767,342) does not match, we need to generate separate geohash values for the coordinates. I am going to use monotonically_increasing_id function to generate unique geohash values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "#create geohash for each coordinate using monotonically_increasing_id\n",
    "coord_df = coord_df.withColumn('geohash_coord', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------+\n",
      "|        lat|         lng|geohash_coord|\n",
      "+-----------+------------+-------------+\n",
      "| 40.4127551|-104.7690948|            0|\n",
      "|  43.450253|  -80.490263|            1|\n",
      "| 39.7185289| -74.9114232|            2|\n",
      "| 40.2688506| -79.8246405|            3|\n",
      "| 30.6381235| -88.2478296|            4|\n",
      "| 45.4375043|-122.8177072|            5|\n",
      "| 37.7038763|-121.8985499|            6|\n",
      "| 36.2916264|-115.1806714|            7|\n",
      "| 39.9424803| -76.7897343|            8|\n",
      "|  38.961574| -76.9149852|            9|\n",
      "|  46.799501| -93.2504123|           10|\n",
      "|47.89985909|-117.7854043|           11|\n",
      "| 40.1515891|   -74.20003|           12|\n",
      "| 37.0519105|-100.9378785|           13|\n",
      "| 34.5617546| -83.2575422|           14|\n",
      "| 41.9748247| -87.9034822|           15|\n",
      "| 33.6445113|-112.2756285|           16|\n",
      "| 40.4125194|-111.7549916|           17|\n",
      "| 41.8793913| -88.0734717|           18|\n",
      "| 37.7619249|-121.4385202|           19|\n",
      "+-----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coord_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using the geohashes, determine if there clusters of people at any point in this dataset. If so, how many people and how close are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#geohash location info + user_ip (to calculate population)\n",
    "geohash_info_df = loc_event_df.select(['geohash', 'user_ip', 'city', 'community_code', 'country_code', 'county_code',\n",
    "                                       'state_code', 'place'])\n",
    "#drop duplicates to get unique user event.\n",
    "geohash_info_df = geohash_info_df.dropDuplicates()\n",
    "\n",
    "total_population = geohash_info_df.count()\n",
    "\n",
    "geohash_ppl_df = geohash_info_df.groupby(['city', 'community_code', 'country_code', 'county_code', 'state_code',\n",
    "                                          'place']).count().orderBy('count', ascending=False)\n",
    "#rename column\n",
    "geohash_ppl_df = geohash_ppl_df.withColumnRenamed('count', 'num_people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+------------+-----------+----------+-------------+----------+\n",
      "|         city|community_code|country_code|county_code|state_code|        place|num_people|\n",
      "+-------------+--------------+------------+-----------+----------+-------------+----------+\n",
      "|      Houston|              |          US|        201|        TX|      Houston|     34905|\n",
      "|     Columbus|              |          US|        049|        OH|     Columbus|     19133|\n",
      "|       Dallas|              |          US|        113|        TX|       Dallas|     18062|\n",
      "|      Atlanta|              |          US|        121|        GA|      Atlanta|     15919|\n",
      "|    Cleveland|              |          US|        035|        OH|    Cleveland|     15123|\n",
      "|    Baltimore|              |          US|        510|        MD|    Baltimore|     14875|\n",
      "| Indianapolis|              |          US|        097|        IN| Indianapolis|     13407|\n",
      "|   Cincinnati|              |          US|        061|        OH|   Cincinnati|     13265|\n",
      "|    Las Vegas|              |          US|        003|        NV|    Las Vegas|     13029|\n",
      "| Philadelphia|              |          US|        101|        PA| Philadelphia|     12699|\n",
      "|      Chicago|              |          US|        031|        IL|      Chicago|     11856|\n",
      "|    Knoxville|              |          US|        093|        TN|    Knoxville|     11648|\n",
      "|    Charlotte|              |          US|        119|        NC|    Charlotte|     11263|\n",
      "|   Birmingham|              |          US|        073|        AL|   Birmingham|     10860|\n",
      "|   Fort Worth|              |          US|        439|        TX|   Fort Worth|     10829|\n",
      "| Jacksonville|              |          US|        031|        FL| Jacksonville|     10636|\n",
      "|      Detroit|              |          US|        163|        MI|      Detroit|      9769|\n",
      "|Oklahoma City|              |          US|        109|        OK|Oklahoma City|      9625|\n",
      "|      Orlando|              |          US|        095|        FL|      Orlando|      9423|\n",
      "|        Omaha|              |          US|        055|        NE|        Omaha|      9062|\n",
      "+-------------+--------------+------------+-----------+----------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geohash_ppl_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not much insights for community and county, so I will see if there are any clusters in the country, state, city, and place level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---------------------+\n",
      "|country_code|sum(num_people)|population_percentage|\n",
      "+------------+---------------+---------------------+\n",
      "|          US|        2352971|   0.7661168985721007|\n",
      "|          CA|         677967|  0.22074304161599587|\n",
      "|          JP|           5788| 0.001884547072163...|\n",
      "|          GU|           5756| 0.001874128014404...|\n",
      "|          MX|           4547| 0.001480482988446...|\n",
      "|          GB|           2436|  7.93150771905662E-4|\n",
      "|          BR|           2213| 7.205429631474671E-4|\n",
      "|          DE|           1640| 5.339767101499531E-4|\n",
      "|          TR|           1627| 5.297439679353498E-4|\n",
      "|          AU|           1572| 5.118362124120282E-4|\n",
      "|          PR|           1245|  4.05366465937007E-4|\n",
      "|          PH|           1219| 3.969009815078004...|\n",
      "|          PK|            980| 3.190836438700938...|\n",
      "|          CO|            959| 3.122461372157347E-4|\n",
      "|          ES|            897| 2.920592128076268E-4|\n",
      "|          RU|            863| 2.809889639386643E-4|\n",
      "|          TH|            847| 2.757794350591526E-4|\n",
      "|          IN|            825| 2.686163328498239...|\n",
      "|          FR|            710| 2.311728440283333...|\n",
      "|          DO|            481| 1.566114619403216E-4|\n",
      "+------------+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#country-level: compute population precentage and show in descending order.\n",
    "geohash_ppl_df.groupby('country_code').agg({'num_people': 'sum'})\\\n",
    "              .withColumn('population_percentage', col('sum(num_people)') / total_population) \\\n",
    "              .orderBy('population_percentage', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------------+\n",
      "|state_code|sum(num_people)|population_percentage|\n",
      "+----------+---------------+---------------------+\n",
      "|        ON|         347008|  0.11298426233885055|\n",
      "|        TX|         249000|   0.0810732931874014|\n",
      "|        FL|         165248|  0.05380401426759722|\n",
      "|        GA|         149717|  0.04874718970336617|\n",
      "|        CA|         147038| 0.047874919211602925|\n",
      "|        OH|         139163|  0.04531085421621824|\n",
      "|        TN|         105931| 0.034490662733472364|\n",
      "|        AL|         105504|  0.03435163343150039|\n",
      "|        NC|          99211|  0.03230266060407743|\n",
      "|        AB|          95831|  0.03120214762828058|\n",
      "|        MI|          83035| 0.027035826906891068|\n",
      "|        BC|          82829| 0.026968754222567354|\n",
      "|        PA|          81647| 0.026583900276593422|\n",
      "|        SC|          71145| 0.023164495758303906|\n",
      "|        QC|          69383| 0.022590796390447677|\n",
      "|        VA|          68852| 0.022417905150758883|\n",
      "|        IL|          63016| 0.020517729491956977|\n",
      "|        MD|          61390| 0.019988311119576597|\n",
      "|        NY|          60972| 0.019852212177599353|\n",
      "|        LA|          48812| 0.015892970229170432|\n",
      "+----------+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#state-level: compute population precentage and show in descending order.\n",
    "geohash_ppl_df.groupby('state_code').agg({'num_people': 'sum'})\\\n",
    "              .withColumn('population_percentage', col('sum(num_people)') / total_population) \\\n",
    "              .orderBy('population_percentage', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---------------------+\n",
      "|        city|sum(num_people)|population_percentage|\n",
      "+------------+---------------+---------------------+\n",
      "|     Toronto|          95486| 0.031089817161816108|\n",
      "|      Ottawa|          37746| 0.012289929817878126|\n",
      "|     Houston|          35102| 0.011429055170538811|\n",
      "|     Calgary|          33923|  0.01104517801122979|\n",
      "|    Edmonton|          28377| 0.009239425063369035|\n",
      "|    Columbus|          24220| 0.007885924341360892|\n",
      "| Mississauga|          23853| 0.007766430772687091|\n",
      "|    Montr√©al|          22907|  0.00745841737768596|\n",
      "|   Vancouver|          20641|  0.00672061785012511|\n",
      "|     Atlanta|          19404| 0.006317856148627859|\n",
      "|      Dallas|          19268| 0.006273575153152009|\n",
      "|   Cleveland|          17077| 0.005560195292213871|\n",
      "|    Winnipeg|          16994| 0.005533170861151403|\n",
      "|   Baltimore|          15288| 0.004977704844373465|\n",
      "|    Hamilton|          14509| 0.004724065907052237|\n",
      "|    Richmond|          14305| 0.004657644413838...|\n",
      "|  Cincinnati|          13846| 0.004508196054107469|\n",
      "|Indianapolis|          13590| 0.004424843592035281|\n",
      "|Philadelphia|          13237| 0.004309908361131054|\n",
      "|   Las Vegas|          13091| 0.004262371410105...|\n",
      "+------------+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#city-level: compute population precentage and show in descending order.\n",
    "geohash_ppl_df.groupby('city').agg({'num_people': 'sum'})\\\n",
    "              .withColumn('population_percentage', col('sum(num_people)') / total_population) \\\n",
    "              .orderBy('population_percentage', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------------+\n",
      "|        place|sum(num_people)|population_percentage|\n",
      "+-------------+---------------+---------------------+\n",
      "|      Houston|          35102| 0.011429055170538811|\n",
      "|     Columbus|          24220| 0.007885924341360892|\n",
      "|      Atlanta|          19404| 0.006317856148627859|\n",
      "|       Dallas|          19268| 0.006273575153152009|\n",
      "|    Cleveland|          17077| 0.005560195292213871|\n",
      "|    Baltimore|          15288| 0.004977704844373465|\n",
      "|   Cincinnati|          13846| 0.004508196054107469|\n",
      "| Indianapolis|          13590| 0.004424843592035281|\n",
      "| Philadelphia|          13237| 0.004309908361131054|\n",
      "|    Las Vegas|          13091| 0.004262371410105...|\n",
      "|  Saint Louis|          12916|  0.00420539218798585|\n",
      "| Jacksonville|          12442| 0.004051059894930315|\n",
      "|   Birmingham|          12001| 0.003907472255188772|\n",
      "|      Chicago|          11856| 0.003860260899718197|\n",
      "|    Knoxville|          11770| 0.003832259681990...|\n",
      "|    Charlotte|          11684| 0.003804258464263446|\n",
      "|Oklahoma City|          11318| 0.003685090491144615|\n",
      "|   Fort Worth|          10829| 0.003525874264764...|\n",
      "|      Detroit|           9794| 0.003188882865371122|\n",
      "|     Richmond|           9777| 0.003183347740936...|\n",
      "+-------------+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#place-level: compute population precentage and show in descending order.\n",
    "geohash_ppl_df.groupby('place').agg({'num_people': 'sum'})\\\n",
    "              .withColumn('population_percentage', col('sum(num_people)') / total_population) \\\n",
    "              .orderBy('population_percentage', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "1. Most of the users are in North America - predominantly in US (76%)\n",
    "2. Ontario had the highest number of users by state\n",
    "3. Toronto had the highest number of users by city\n",
    "4. In US, the users are more scattered aroud the regions, where in Canada the users are concentrated in ON, QC, BC, and AB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write any findings into a local parquet format file for later use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Saving loc_event_df_idfa, coord_df, geohash_ppl_df to a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_event_df_idfa.write.parquet('loc_event_df_idfa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geohash_ppl_df.write.parquet('geohash_ppl_df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
